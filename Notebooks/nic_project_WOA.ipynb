{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models, Model\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import kagglehub\n",
        "import math\n",
        "import time\n",
        "import gc"
      ],
      "metadata": {
        "id": "NmLFPWQwxXgU"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "os.environ['PYTHONHASHSEED'] = str(SEED)"
      ],
      "metadata": {
        "id": "w0MqejmExYiT"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Downloading dataset...\")\n",
        "path = kagglehub.dataset_download(\"bhaveshmittal/melanoma-cancer-dataset\")\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ljwe29isxZYl",
        "outputId": "e0aa2c1d-71a9-4e25-b30a-346768420ed0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading dataset...\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/bhaveshmittal/melanoma-cancer-dataset?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 79.4M/79.4M [00:04<00:00, 18.0MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/bhaveshmittal/melanoma-cancer-dataset/versions/1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_WIDTH = 64\n",
        "IMAGE_HEIGHT = 64\n",
        "BATCH_SIZE = 32\n",
        "INPUT_SHAPE = (IMAGE_WIDTH, IMAGE_HEIGHT, 3)"
      ],
      "metadata": {
        "id": "Rqp1qSqWxadQ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1.0 / 255,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    validation_split=0.4\n",
        ")\n",
        "\n",
        "test_datagen = ImageDataGenerator(\n",
        "    rescale=1.0 / 255,\n",
        "    validation_split=0.4\n",
        ")\n",
        "\n",
        "# Load Data\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    path + '/train',\n",
        "    target_size=(IMAGE_WIDTH, IMAGE_HEIGHT),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='binary',\n",
        "    subset='training',\n",
        "    seed=SEED\n",
        ")\n",
        "\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "    path + '/train',\n",
        "    target_size=(IMAGE_WIDTH, IMAGE_HEIGHT),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='binary',\n",
        "    subset='validation',\n",
        "    seed=SEED\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJdJWkDxxdao",
        "outputId": "5162967e-5f63-4d40-95b6-e2c75f4b508a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 7128 images belonging to 2 classes.\n",
            "Found 4751 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(\n",
        "    filters1, filters2, dropout1, dropout2,\n",
        "    learning_rate, activation, last_activation,\n",
        "    keep_model=False\n",
        "):\n",
        "    model = models.Sequential([\n",
        "        layers.Conv2D(filters=filters1, kernel_size=(3, 3), activation=activation,\n",
        "                      input_shape=(IMAGE_WIDTH, IMAGE_HEIGHT, 3)),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "        layers.Conv2D(filters=filters2, kernel_size=(3, 3), activation=activation),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "        layers.Conv2D(filters=128, kernel_size=(3, 3), activation=activation),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(units=128, activation=activation),\n",
        "        layers.Dropout(dropout1),\n",
        "        layers.Dense(units=64, activation=activation),\n",
        "        layers.Dropout(dropout2),\n",
        "        layers.Dense(units=1, activation=last_activation)\n",
        "    ])\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=\"binary_crossentropy\",\n",
        "        metrics=[\"accuracy\"]\n",
        "    )\n",
        "\n",
        "    history = model.fit(\n",
        "        train_generator,\n",
        "        steps_per_epoch=train_generator.samples // BATCH_SIZE,\n",
        "        epochs=4,\n",
        "        validation_data=validation_generator,\n",
        "        validation_steps=validation_generator.samples // BATCH_SIZE,\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    val_acc = history.history[\"val_accuracy\"][-1]\n",
        "\n",
        "    # ===== MEMORY SAFE CLEANUP =====\n",
        "    if not keep_model:\n",
        "        tf.keras.backend.clear_session()\n",
        "        del model\n",
        "        del history\n",
        "        del optimizer\n",
        "        return val_acc, None, None\n",
        "    # ===============================\n",
        "\n",
        "    return val_acc, model, history"
      ],
      "metadata": {
        "id": "ga3qPHITxfOz"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "search_space = {}\n",
        "search_space['filters1'] = [32, 64, 128]\n",
        "search_space['filters2'] = [32,64,128]\n",
        "search_space['dropout1'] = [0.1, 0.3, 0.5, 0.7]\n",
        "search_space['dropout2'] = [0.1, 0.3, 0.5, 0.7]\n",
        "search_space['learning_rate'] = [0.0001, 0.001, 0.01, 0.1]\n",
        "search_space['activation'] = ['relu', 'elu', 'gelu']\n",
        "search_space['last_activation'] = ['sigmoid']"
      ],
      "metadata": {
        "id": "fiALFP5xxzzk"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _immediate_neighbours(index, dimension, state):\n",
        "    neighbours = []\n",
        "    vals = search_space[dimension]\n",
        "    new_state_base = list(state)  # Convert tuple to list for modification\n",
        "\n",
        "    if index == 0:\n",
        "        neigh_state = new_state_base.copy()\n",
        "        neigh_state[list(search_space.keys()).index(dimension)] = vals[1]\n",
        "        neighbours.append(tuple(neigh_state))\n",
        "\n",
        "        neigh_state = new_state_base.copy()\n",
        "        neigh_state[list(search_space.keys()).index(dimension)] = vals[2]\n",
        "        neighbours.append(tuple(neigh_state))\n",
        "    elif index == len(vals) - 1:\n",
        "        neigh_state = new_state_base.copy()\n",
        "        neigh_state[list(search_space.keys()).index(dimension)] = vals[-2]\n",
        "        neighbours.append(tuple(neigh_state))\n",
        "\n",
        "        neigh_state = new_state_base.copy()\n",
        "        neigh_state[list(search_space.keys()).index(dimension)] = vals[-3]\n",
        "        neighbours.append(tuple(neigh_state))\n",
        "    else:\n",
        "        neigh_state = new_state_base.copy()\n",
        "        neigh_state[list(search_space.keys()).index(dimension)] = vals[index - 1]\n",
        "        neighbours.append(tuple(neigh_state))\n",
        "\n",
        "        neigh_state = new_state_base.copy()\n",
        "        neigh_state[list(search_space.keys()).index(dimension)] = vals[index + 1]\n",
        "        neighbours.append(tuple(neigh_state))\n",
        "\n",
        "    return neighbours\n",
        "\n",
        "def get_neighbours(state):\n",
        "    neighbours = []\n",
        "    state_dict = {k: state[i] for i, k in enumerate(search_space.keys())}\n",
        "\n",
        "    for k in search_space.keys():\n",
        "        if k in ['activation', 'last_activation']:\n",
        "            continue\n",
        "        index = search_space[k].index(state_dict[k])\n",
        "        new_neighbours = _immediate_neighbours(index, k, state)\n",
        "        neighbours.extend(new_neighbours)\n",
        "\n",
        "    # Add a neighbor with random activation and last_activation\n",
        "    new_state = list(state)\n",
        "    new_state[5] = np.random.choice(search_space['activation'])\n",
        "    new_state[6] = np.random.choice(search_space['last_activation'])\n",
        "    neighbours.append(tuple(new_state))\n",
        "\n",
        "    return neighbours"
      ],
      "metadata": {
        "id": "KXRdcljyx3YI"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Pj4AJRJ_xSod"
      },
      "outputs": [],
      "source": [
        "def whale_search(max_iter=10, population_size=3, b=1.0):\n",
        "    print(f\"  -> WOA Start (Pop={population_size}, Iter={max_iter}, b={b})\")\n",
        "    keys = list(search_space.keys())\n",
        "    population = []\n",
        "    for _ in range(population_size):\n",
        "        w = (\n",
        "            random.choice(search_space[\"filters1\"]),\n",
        "            random.choice(search_space[\"filters2\"]),\n",
        "            random.choice(search_space[\"dropout1\"]),\n",
        "            random.choice(search_space[\"dropout2\"]),\n",
        "            random.choice(search_space[\"learning_rate\"]),\n",
        "            random.choice(search_space[\"activation\"]),\n",
        "            random.choice(search_space[\"last_activation\"])\n",
        "        )\n",
        "        population.append(w)\n",
        "\n",
        "    scores = []\n",
        "    # --- Evaluate Initial Population ---\n",
        "    for w in population:\n",
        "        # keep_model=False ensures we don't return the heavy object\n",
        "        acc, _, _ = evaluate_model(*w, keep_model=False)\n",
        "        scores.append(acc)\n",
        "        # CLEANUP\n",
        "        tf.keras.backend.clear_session()\n",
        "        gc.collect()\n",
        "\n",
        "    best_index = int(np.argmax(scores))\n",
        "    best_state = population[best_index]\n",
        "    best_score = scores[best_index]\n",
        "\n",
        "    for t in range(max_iter):\n",
        "        a = 2 - 2 * (t / max_iter)\n",
        "        new_population = []\n",
        "\n",
        "        for i in range(population_size):\n",
        "            whale = population[i]\n",
        "            p = random.random()\n",
        "            # Convert categorical params to indices for math operations\n",
        "            whale_indices = [\n",
        "                search_space[\"filters1\"].index(whale[0]),\n",
        "                search_space[\"filters2\"].index(whale[1]),\n",
        "                search_space[\"dropout1\"].index(whale[2]),\n",
        "                search_space[\"dropout2\"].index(whale[3]),\n",
        "                search_space[\"learning_rate\"].index(whale[4])\n",
        "            ]\n",
        "\n",
        "            best_indices = [\n",
        "                search_space[\"filters1\"].index(best_state[0]),\n",
        "                search_space[\"filters2\"].index(best_state[1]),\n",
        "                search_space[\"dropout1\"].index(best_state[2]),\n",
        "                search_space[\"dropout2\"].index(best_state[3]),\n",
        "                search_space[\"learning_rate\"].index(best_state[4])\n",
        "            ]\n",
        "\n",
        "            updated_indices = []\n",
        "\n",
        "            # Update numeric parameters (first 5 params)\n",
        "            for dim in range(len(whale_indices)):\n",
        "                r1 = random.random()\n",
        "                r2 = random.random()\n",
        "                A = 2 * a * r1 - a\n",
        "                C = 2 * r2\n",
        "\n",
        "                val = 0\n",
        "\n",
        "                if p < 0.5:\n",
        "                    if abs(A) < 1:\n",
        "                        # Encircling prey\n",
        "                        D = abs(C * best_indices[dim] - whale_indices[dim])\n",
        "                        val = best_indices[dim] - A * D\n",
        "                    else:\n",
        "                        # Search for prey (exploration)\n",
        "                        rand_idx = random.randint(0, population_size - 1)\n",
        "                        rand_whale = population[rand_idx]\n",
        "                        rand_val_idx = random.randint(0, len(search_space[keys[dim]]) - 1)\n",
        "                        D = abs(C * rand_val_idx - whale_indices[dim])\n",
        "                        val = rand_val_idx - A * D\n",
        "                else:\n",
        "                    # Spiral updating\n",
        "                    D_prime = abs(best_indices[dim] - whale_indices[dim])\n",
        "                    l = random.uniform(-1, 1)\n",
        "                    val = D_prime * math.exp(b * l) * math.cos(2 * math.pi * l) + best_indices[dim]\n",
        "\n",
        "                val = int(round(val))\n",
        "                val = max(0, min(val, len(search_space[keys[dim]]) - 1))\n",
        "                updated_indices.append(val)\n",
        "\n",
        "            new_state = (\n",
        "                search_space[\"filters1\"][updated_indices[0]],\n",
        "                search_space[\"filters2\"][updated_indices[1]],\n",
        "                search_space[\"dropout1\"][updated_indices[2]],\n",
        "                search_space[\"dropout2\"][updated_indices[3]],\n",
        "                search_space[\"learning_rate\"][updated_indices[4]],\n",
        "                random.choice(search_space[\"activation\"]),      # Randomly mutated\n",
        "                random.choice(search_space[\"last_activation\"])  # Randomly mutated\n",
        "            )\n",
        "            new_population.append(new_state)\n",
        "\n",
        "        # --- Evaluate New Positions ---\n",
        "        new_scores = []\n",
        "        for w in new_population:\n",
        "            acc, _, _ = evaluate_model(*w, keep_model=False)\n",
        "            new_scores.append(acc)\n",
        "\n",
        "            # CLEANUP\n",
        "            tf.keras.backend.clear_session()\n",
        "            gc.collect()\n",
        "\n",
        "        # Check for improvement in this batch\n",
        "        iteration_best_index = int(np.argmax(new_scores))\n",
        "        iteration_best_score = new_scores[iteration_best_index]\n",
        "        iteration_best_state = new_population[iteration_best_index]\n",
        "\n",
        "        if iteration_best_score > best_score:\n",
        "            best_score = iteration_best_score\n",
        "            best_state = iteration_best_state\n",
        "\n",
        "        population = new_population\n",
        "\n",
        "    print(f\"  -> WOA Finished. Best Score: {best_score:.4f}\")\n",
        "\n",
        "    return best_state, best_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "best_state, best_score = whale_search()\n",
        "end_time = time.time()\n",
        "print(f\"Total time taken: {end_time - start_time:.2f} seconds\")\n",
        "print(f\"Best State: {best_state}\")\n",
        "print(f\"Best Score: {best_score:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGl1rBTEyB6h",
        "outputId": "d8d2503f-fc87-4001-aef8-890198502df5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> WOA Start (Pop=3, Iter=10, b=1.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/epoch_iterator.py:116: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self._interrupted_warning()\n",
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/epoch_iterator.py:116: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self._interrupted_warning()\n",
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/epoch_iterator.py:116: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self._interrupted_warning()\n",
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> WOA Finished. Best Score: 0.8161\n",
            "Total time taken: 2185.08 seconds\n",
            "Best State: (32, 128, 0.7, 0.1, 0.0001, 'elu', 'sigmoid')\n",
            "Best Score: 0.8161\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_generator = test_datagen.flow_from_directory(\n",
        "    path + '/test',\n",
        "    target_size=(IMAGE_WIDTH, IMAGE_HEIGHT),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='binary',\n",
        "    subset='training',\n",
        "    shuffle=False,\n",
        "    seed=SEED\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FWZtdpEI0ZyZ",
        "outputId": "5678b8de-6452-4f99-bd23-98bbb5661fd2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1200 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filters1, filters2, dropout1, dropout2, learning_rate, activation, last_activation = best_state\n",
        "\n",
        "optimized_model = models.Sequential([\n",
        "    layers.Conv2D(filters=filters1, kernel_size=(3, 3), activation=activation, input_shape=(IMAGE_WIDTH, IMAGE_HEIGHT, 3)),\n",
        "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "    layers.Conv2D(filters=filters2, kernel_size=(3, 3), activation=activation),\n",
        "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "    layers.Conv2D(filters=128, kernel_size=(3, 3), activation=activation),\n",
        "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(units=128, activation=activation),\n",
        "    layers.Dropout(dropout1),\n",
        "    layers.Dense(units=64, activation=activation),\n",
        "    layers.Dropout(dropout2),\n",
        "    layers.Dense(units=1, activation=last_activation)\n",
        "])\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "optimized_model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-6)\n",
        "\n",
        "history = optimized_model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_generator.samples // BATCH_SIZE,\n",
        "    epochs=20,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=validation_generator.samples // BATCH_SIZE,\n",
        "    callbacks=[early_stopping, reduce_lr]\n",
        ")\n",
        "\n",
        "test_loss, test_acc = optimized_model.evaluate(test_generator)\n",
        "print(f\"Test accuracy: {test_acc}\")\n",
        "\n",
        "optimized_model.save('WOA_optimized_melanoma_model.keras')\n",
        "print(\"Final model saved as WOA_optimized_melanoma_model.keras\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRfhmV50yZSH",
        "outputId": "12c55d7e-0a6e-45d6-8126-8a6fd67cd2e0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 106ms/step - accuracy: 0.6596 - loss: 0.6062 - val_accuracy: 0.6634 - val_loss: 0.6635 - learning_rate: 1.0000e-04\n",
            "Epoch 2/20\n",
            "\u001b[1m  1/222\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 0.5938 - loss: 0.7916"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/epoch_iterator.py:116: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self._interrupted_warning()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 33ms/step - accuracy: 0.5938 - loss: 0.7916 - val_accuracy: 0.7238 - val_loss: 0.5158 - learning_rate: 1.0000e-04\n",
            "Epoch 3/20\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 95ms/step - accuracy: 0.8126 - loss: 0.4153 - val_accuracy: 0.8081 - val_loss: 0.4461 - learning_rate: 1.0000e-04\n",
            "Epoch 4/20\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 37ms/step - accuracy: 0.9062 - loss: 0.3269 - val_accuracy: 0.8051 - val_loss: 0.4575 - learning_rate: 1.0000e-04\n",
            "Epoch 5/20\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 91ms/step - accuracy: 0.8155 - loss: 0.3958 - val_accuracy: 0.7720 - val_loss: 0.4619 - learning_rate: 1.0000e-04\n",
            "Epoch 6/20\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 35ms/step - accuracy: 0.8750 - loss: 0.3203 - val_accuracy: 0.7844 - val_loss: 0.4434 - learning_rate: 1.0000e-04\n",
            "Epoch 7/20\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 96ms/step - accuracy: 0.8372 - loss: 0.3605 - val_accuracy: 0.7804 - val_loss: 0.4486 - learning_rate: 1.0000e-04\n",
            "Epoch 8/20\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 35ms/step - accuracy: 0.6562 - loss: 0.6140 - val_accuracy: 0.7787 - val_loss: 0.4490 - learning_rate: 1.0000e-04\n",
            "Epoch 9/20\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 93ms/step - accuracy: 0.8337 - loss: 0.3658 - val_accuracy: 0.8174 - val_loss: 0.3983 - learning_rate: 1.0000e-04\n",
            "Epoch 10/20\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 37ms/step - accuracy: 0.7812 - loss: 0.4233 - val_accuracy: 0.8178 - val_loss: 0.3992 - learning_rate: 1.0000e-04\n",
            "Epoch 11/20\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 96ms/step - accuracy: 0.8358 - loss: 0.3614 - val_accuracy: 0.8165 - val_loss: 0.3938 - learning_rate: 1.0000e-04\n",
            "Epoch 12/20\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 33ms/step - accuracy: 0.7812 - loss: 0.4285 - val_accuracy: 0.8182 - val_loss: 0.3985 - learning_rate: 1.0000e-04\n",
            "Epoch 13/20\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 96ms/step - accuracy: 0.8529 - loss: 0.3354 - val_accuracy: 0.8247 - val_loss: 0.3945 - learning_rate: 1.0000e-04\n",
            "Epoch 14/20\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 37ms/step - accuracy: 0.8438 - loss: 0.2971 - val_accuracy: 0.8262 - val_loss: 0.3859 - learning_rate: 1.0000e-04\n",
            "Epoch 15/20\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 92ms/step - accuracy: 0.8437 - loss: 0.3401 - val_accuracy: 0.8264 - val_loss: 0.3888 - learning_rate: 1.0000e-04\n",
            "Epoch 16/20\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 37ms/step - accuracy: 0.8125 - loss: 0.4495 - val_accuracy: 0.8252 - val_loss: 0.3847 - learning_rate: 1.0000e-04\n",
            "Epoch 17/20\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 96ms/step - accuracy: 0.8534 - loss: 0.3325 - val_accuracy: 0.8326 - val_loss: 0.3907 - learning_rate: 1.0000e-04\n",
            "Epoch 18/20\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 34ms/step - accuracy: 0.7812 - loss: 0.5022 - val_accuracy: 0.8285 - val_loss: 0.3851 - learning_rate: 1.0000e-04\n",
            "Epoch 19/20\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 98ms/step - accuracy: 0.8516 - loss: 0.3401 - val_accuracy: 0.8239 - val_loss: 0.3937 - learning_rate: 1.0000e-04\n",
            "\u001b[1m 4/38\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.8841 - loss: 0.3048"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - accuracy: 0.8634 - loss: 0.2999\n",
            "Test accuracy: 0.8075000047683716\n",
            "Final model saved as WOA_optimized_melanoma_model.keras\n"
          ]
        }
      ]
    }
  ]
}